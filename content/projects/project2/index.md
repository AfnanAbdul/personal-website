---
title: "Accelerating Transformer Attention with Custom CUDA Kernels"
date: 2025-02-28
lastmod: 2025-02-28
tags: ["Transformers", "CUDA", "GPU Optimization", "AI", "Deep Learning", "Python", "C++", "PyTorch", "NVIDIA Nsight Systems", "Profiling Tools"]
description: "An in-depth performance analysis of PyTorch's MultiheadAttention module, identifying computational bottlenecks and optimizing CUDA kernels for efficient GPU utilization."
summary: "As part of my **GPU Architectures** course, this project explores profiling and optimizing attention mechanisms in transformers using custom CUDA extensions. The focus is on reducing inference and training latency through kernel-level enhancements, improving GPU resource utilization for deep learning workloads."
showToc: false
weight: 2
cover:
    image: ""  # Add a project image here if available
    alt: "Optimizing Transformer Attention with CUDA"
    relative: false
---

---

##### Links

+ [Paper](#)
+ [Online appendix](#)
+ [Code and data](#)

---

##### Overview


---

#### Related material

+ [Presentation slides](#)