---
title: "Comparative Study of Large Language Model Evaluation Frameworks"
date: 2025-02-28
lastmod: 2025-02-28
tags: ["LLMs", "AI", "Bias Detection", "Python", "Claude", "RAGAS", "promptfoo", "DeepEval", "TruLens", "UVA", "Data Science Capstone"]
description: "A comprehensive analysis of evaluation frameworks for LLMs, focusing on bias detection, response quality, and robustness."
summary: "As part of my **capstone project in the Masterâ€™s in Data Science program at the University of Virginia**, this research evaluates various LLM evaluation frameworks, emphasizing bias detection, response quality assessment, and robustness testing. The study leverages multiple datasets and methodologies to benchmark state-of-the-art approaches for ethical and reliable AI assessment."
showToc: false
weight: 1
cover:
    image: ""  # You can add a project image here if you have one
    alt: "LLM Evaluation Framework Comparison"
    relative: false
---

---

##### Links

+ [Paper](#)
+ [Online appendix](#)
+ [Code and data](#)

---

##### Overview

As part of my UVA capstone project, I collaborated with an industry sponsor to analyze and compare evaluation frameworks for large language models (LLMs). This study focused on key metrics such as response accuracy, retrieval effectiveness, bias detection, toxicity, hallucination, and tone identification. I led the bias detection evaluation, implementing Counterfactual Data Testing with the WinoBias dataset to measure LLM response consistency across sensitive attributes. Additionally, I developed and applied custom bias detection methods using promptfoo, DeepEval, and RAGAS, conducting comparative analyses on 1,500+ sentence pairs from the CrowS-Pairs dataset. By integrating counterfactual data testing with contextual sensitivity analysis, our research aimed to enhance gender bias evaluation in LLMs and contribute to more ethical AI assessment methodologies.

---

#### Related material

+ [Presentation slides](#)
