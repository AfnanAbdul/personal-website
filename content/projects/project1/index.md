---
title: "Comparative Study of LLM Evaluation Frameworks (Deloitte-Anthropic Alliance)"
date: 2025-05-23
lastmod: 2025-05-23
tags: ["LLMs", "LLM-as-a-Judge","AI Ethics", "Bias Detection", "Model Evaluation", "Python", "Claude", "RAGAS", "promptfoo", "DeepEval", "TruLens", "UVA", "Data Science Capstone", "Anthropic", "Deloitte"]
description: "A comprehensive analysis of evaluation frameworks for LLMs, focusing on bias detection, response quality, and robustness - conducted in collaboration with Deloitte's Anthropic Alliance."
summary: "In collaboration with Deloitte's Anthropic Alliance, this capstone research for the M.S. in Data Science at the University of Virginia critically examines leading frameworks for evaluating large language models (LLMs). The study leverages multiple datasets and methodologies to benchmark state-of-the-art approaches for ethical and reliable AI assessment. This comprehensive research evaluated and compared multiple LLM evaluation frameworks across eight critical metrics: toxicity detection, bias detection, hallucination detection, summarization quality, tone identification, readability assessment, retrieval accuracy, and response accuracy."
showToc: false
weight: 1
cover:
    image: "project1.png" 
    alt: "LLM Evaluation Framework Comparison"
    relative: false
---

---

##### Links

+ [Paper (Extended Version)](LLM_as_a_judge_Deloitte.pdf)
+ [SIEDS Conference Paper (IEEE)](https://ieeexplore.ieee.org/document/11021089)
+ [Code](https://github.com/AfnanAbdul/LLM-eval-framework-comparison)
+ [Presentation Slides](1003am_deloitte_anthropic_llm.pdf)
+ [LLM Evaluation Framework Leaderboard](https://llm-evaluation-framework-leaderboard.vercel.app)

---

##### Overview

üèÜ Winner of ["Most Innovative Analytical Solution" Award](https://bit.ly/3RTDgCo)

As part of my UVA capstone project, I collaborated with Deloitte's Anthropic Alliance to analyze and compare evaluation frameworks for large language models (LLMs). This award-winning study focused on key metrics such as response accuracy, retrieval effectiveness, bias detection, toxicity, hallucination, and tone identification. I led the bias detection evaluation, implementing Counterfactual Data Testing with the WinoBias dataset to measure LLM response consistency across sensitive attributes. Additionally, I developed and applied custom bias detection methods using promptfoo, DeepEval, and RAGAS, conducting comparative analyses on 1,500+ sentence pairs from the CrowS-Pairs dataset. By integrating counterfactual data testing with contextual sensitivity analysis, our research aimed to enhance bias evaluation in LLMs and contribute to more ethical AI assessment methodologies.

**Publications:** This research was presented at the UVA 2025 Systems and Information Engineering Design Symposium (SIEDS), where we published a condensed 6-page version of our findings in the IEEE conference proceedings.

---

#### Related material

+ [üèõÔ∏è UVA Master‚Äôs in Data Science Students Showcase Real-World Solutions in 2025 Capstone Presentations](https://bit.ly/3RTDgCo)
+ [Deloitte and Anthropic Collaborate to Bring Safe, Reliable and Trusted AI to Commercial and Government Organizations](https://www.prnewswire.com/news-releases/deloitte-and-anthropic-collaborate-to-bring-safe-reliable-and-trusted-ai-to-commercial-and-government-organizations-302210769.html?tc)
+ [Deloitte Collaborates With Anthropic to Advance Enterprise AI Capabilities Through AI Training and Certification Program](https://www2.deloitte.com/us/en/pages/about-deloitte/articles/press-releases/deloitte-and-anthropic-launch-certification-program.html)
